"use strict";(self.webpackChunkstade_pheno=self.webpackChunkstade_pheno||[]).push([[8271],{4907:(e,n,t)=>{t.d(n,{A:()=>r});const r=t.p+"assets/images/yolov8-architecture-detail-79a92d3dd7f81051b4a9124e436446c9.png"},8453:(e,n,t)=>{t.d(n,{R:()=>a,x:()=>l});var r=t(6540);const s={},i=r.createContext(s);function a(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:a(e.components),r.createElement(i.Provider,{value:n},e.children)}},9114:(e,n,t)=>{t.r(n),t.d(n,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>a,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"MLops/model-training","title":"Entra\xeenement du mod\xe8le","description":"Cette section d\xe9taille le processus d\'entra\xeenement du mod\xe8le de d\xe9tection pour le projet Ph\xe9noRendement. Nous avons adopt\xe9 une approche en deux \xe9tapes pour maximiser la pr\xe9cision tout en optimisant l\'utilisation des ressources.","source":"@site/docs/MLops/model-training.md","sourceDirName":"MLops","slug":"/MLops/model-training","permalink":"/stade-pheno/ar/docs/MLops/model-training","draft":false,"unlisted":false,"editUrl":"https://github.com/fatimabelgazem/stade-pheno/tree/main/docs/MLops/model-training.md","tags":[],"version":"current","frontMatter":{},"sidebar":"tutorialSidebar","previous":{"title":"\ud83e\uddea 3 - \xc9valuation du Mod\xe8le","permalink":"/stade-pheno/ar/docs/MLops/model-evaluation"},"next":{"title":"\ud83d\udce1 5 - Monitoring du Mod\xe8le","permalink":"/stade-pheno/ar/docs/MLops/monitoring"}}');var s=t(4848),i=t(8453);const a={},l="Entra\xeenement du mod\xe8le",o={},d=[{value:"\ud83e\udde0 2.1 - Strat\xe9gie d&#39;entra\xeenement",id:"-21---strat\xe9gie-dentra\xeenement",level:2},{value:"\ud83d\udd39 Avantages de cette architecture",id:"-avantages-de-cette-architecture",level:3},{value:"\ud83d\udd39 Fonctionnalit\xe9s cl\xe9s",id:"-fonctionnalit\xe9s-cl\xe9s",level:3},{value:"\u2699\ufe0f 2.4 - Configuration d&#39;entra\xeenement",id:"\ufe0f-24---configuration-dentra\xeenement",level:2},{value:"\ud83d\udd27 Hyperparam\xe8tres principaux :",id:"-hyperparam\xe8tres-principaux-",level:3},{value:"\ud83d\ude80 2.5 - Processus d&#39;entra\xeenement",id:"-25---processus-dentra\xeenement",level:2},{value:"\ud83d\udcca Suivi des performances :",id:"-suivi-des-performances-",level:3}];function c(e){const n={code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"entra\xeenement-du-mod\xe8le",children:"Entra\xeenement du mod\xe8le"})}),"\n",(0,s.jsxs)(n.p,{children:["Cette section d\xe9taille le processus d'entra\xeenement du mod\xe8le de d\xe9tection pour le projet ",(0,s.jsx)(n.strong,{children:"Ph\xe9noRendement"}),". Nous avons adopt\xe9 une approche en deux \xe9tapes pour maximiser la pr\xe9cision tout en optimisant l'utilisation des ressources."]}),"\n",(0,s.jsx)(n.h2,{id:"-21---strat\xe9gie-dentra\xeenement",children:"\ud83e\udde0 2.1 - Strat\xe9gie d'entra\xeenement"}),"\n",(0,s.jsx)(n.p,{children:"Notre strat\xe9gie d'entra\xeenement repose sur le transfer learning (apprentissage par transfert) :"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\ud83e\udd47 \xc9tape 1 :"}),"  Pr\xe9-entra\xeenement sur un dataset g\xe9n\xe9rique de plantes"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\ud83e\udd48 \xc9tape 2 :"}),"  Fine-tuning sur notre dataset sp\xe9cifique d'orangers"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Cette approche nous permet de b\xe9n\xe9ficier des connaissances g\xe9n\xe9rales sur la d\xe9tection d'\xe9l\xe9ments de plantes, tout en sp\xe9cialisant notre mod\xe8le pour notre cas d'usage pr\xe9cis."}),"\n",(0,s.jsx)(n.h1,{id:"-22---architecture-yolov8",children:"\ud83e\udde0 2.2 - Architecture YOLOv8"}),"\n",(0,s.jsx)(n.p,{children:"YOLOv8 est bas\xe9 sur une architecture de r\xe9seau neuronal avanc\xe9e qui se compose de trois composants principaux :"}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udd39 ",(0,s.jsx)(n.strong,{children:"Backbone (R\xe9seau dorsal) :"})]}),"\n",(0,s.jsxs)(n.p,{children:["Le backbone est le r\xe9seau neuronal convolutif ",(0,s.jsx)(n.strong,{children:"(CNN)"})," responsable de l'extraction des caract\xe9ristiques \xe0 partir de l'image d'entr\xe9e. ",(0,s.jsx)(n.strong,{children:"YOLOv8"})," utilise un backbone ",(0,s.jsx)(n.strong,{children:"CSPDarknet53"})," personnalis\xe9, qui emploie des connexions partielles entre \xe9tapes ",(0,s.jsx)(n.strong,{children:"(cross-stage partial connections)"})," pour am\xe9liorer la circulation de l'information entre les couches et augmenter la pr\xe9cision."]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udd39 ",(0,s.jsx)(n.strong,{children:"Neck (Cou) :"})]}),"\n",(0,s.jsxs)(n.p,{children:["Le neck, \xe9galement connu sous le nom d'extracteur de caract\xe9ristiques, fusionne les cartes de caract\xe9ristiques provenant de diff\xe9rentes \xe9tapes du backbone pour capturer des informations \xe0 diverses \xe9chelles. L'architecture ",(0,s.jsx)(n.strong,{children:"YOLOv8"})," utilise un module ",(0,s.jsx)(n.strong,{children:"C2f"})," innovant au lieu du traditionnel ",(0,s.jsx)(n.strong,{children:"Feature Pyramid Network (FPN)"}),". Ce module combine des caract\xe9ristiques s\xe9mantiques de haut niveau avec des informations spatiales de bas niveau, ce qui conduit \xe0 une meilleure pr\xe9cision de d\xe9tection, en particulier pour les petits objets."]}),"\n",(0,s.jsxs)(n.p,{children:["\ud83d\udd39 ",(0,s.jsx)(n.strong,{children:"Head (T\xeate) :"}),"\r\nLa t\xeate est responsable des pr\xe9dictions. ",(0,s.jsx)(n.strong,{children:"YOLOv8"})," emploie plusieurs modules de d\xe9tection qui pr\xe9disent les bo\xeetes englobantes (bounding boxes), les scores d'objectivit\xe9 (objectness scores) et les probabilit\xe9s de classe pour chaque cellule de la grille dans la carte de caract\xe9ristiques. Ces pr\xe9dictions sont ensuite agr\xe9g\xe9es pour obtenir les d\xe9tections finales."]}),"\n",(0,s.jsx)("p",{align:"center",children:(0,s.jsx)("img",{src:t(4907).A,alt:"Architecture d\xe9taill\xe9e de YOLOv8",width:"600px"})}),"\n",(0,s.jsx)(n.h3,{id:"-avantages-de-cette-architecture",children:"\ud83d\udd39 Avantages de cette architecture"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Efficacit\xe9"})," : Traitement rapide des images, permettant une d\xe9tection en temps r\xe9el"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Pr\xe9cision"})," : Tr\xe8s bonne performance sur les objets de diff\xe9rentes tailles"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Flexibilit\xe9"})," : Plusieurs variantes (nano, small, medium, large, x-large) adapt\xe9es \xe0 diff\xe9rents contextes"]}),"\n"]}),"\n",(0,s.jsx)(n.h3,{id:"-fonctionnalit\xe9s-cl\xe9s",children:"\ud83d\udd39 Fonctionnalit\xe9s cl\xe9s"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"D\xe9tection multi-\xe9chelle"})," : Capacit\xe9 \xe0 d\xe9tecter des objets de tailles vari\xe9es"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"M\xe9canisme d'attention"})," : Pour se concentrer sur les r\xe9gions importantes de l'image"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Augmentation d'ancrage"})," : Pour am\xe9liorer la stabilit\xe9 de l'entra\xeenement"]}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Cette architecture sophistiqu\xe9e fait de YOLOv8 un excellent choix pour notre t\xe2che de d\xe9tection des stades ph\xe9nologiques des orangers, offrant un bon \xe9quilibre entre vitesse et pr\xe9cision."}),"\n",(0,s.jsx)(n.h1,{id:"-23---mod\xe8le-de-base",children:"\ud83d\udd0d 2.3 - Mod\xe8le de base"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\ud83d\udcca Architecture :"}),"  YOLOv8m (medium)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\ud83c\udf31 Pr\xe9-entra\xeenement initial :"}),'  Sur le dataset "Nature3: Leaf, Flower, and Fruit Detection"']}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"\ud83d\udcc8 Avantages :"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Architecture efficace pour la d\xe9tection d'objets en temps r\xe9el."}),"\n",(0,s.jsx)(n.li,{children:'Version "medium" offrant un bon \xe9quilibre entre pr\xe9cision et vitesse d\'inf\xe9rence.'}),"\n",(0,s.jsx)(n.li,{children:"Capacit\xe9 prouv\xe9e sur les t\xe2ches de d\xe9tection d'\xe9l\xe9ments naturels."}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"\ufe0f-24---configuration-dentra\xeenement",children:"\u2699\ufe0f 2.4 - Configuration d'entra\xeenement"}),"\n",(0,s.jsx)(n.p,{children:"Nous avons utilis\xe9 MLflow pour suivre l'\xe9volution de notre entra\xeenement et g\xe9rer les exp\xe9riences :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'pythonimport mlflow\r\nimport os\r\nimport pandas as pd\r\n\r\n# Initialisation de MLflow\r\nmlflow.set_tracking_uri("file:///kaggle/working/mlruns")\r\nmlflow.set_experiment("YOLOv8_StadePheno1")\n'})}),"\n",(0,s.jsx)(n.h3,{id:"-hyperparam\xe8tres-principaux-",children:"\ud83d\udd27 Hyperparam\xe8tres principaux :"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Param\xe8tre"}),(0,s.jsx)(n.th,{children:"Valeur"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"model_path"}),(0,s.jsx)(n.td,{children:"/kaggle/input/modellast43/tensorflow2/default/1/last.pt"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"epochs"}),(0,s.jsx)(n.td,{children:"100"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"imgsz"}),(0,s.jsx)(n.td,{children:"800"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"task"}),(0,s.jsx)(n.td,{children:"detect"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"data"}),(0,s.jsx)(n.td,{children:"/kaggle/working/datasets/Stade_Pheno_Dataset-3/data.yaml"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:"resume"}),(0,s.jsx)(n.td,{children:"True"})]})]})]}),"\n",(0,s.jsx)(n.h2,{id:"-25---processus-dentra\xeenement",children:"\ud83d\ude80 2.5 - Processus d'entra\xeenement"}),"\n",(0,s.jsx)(n.p,{children:"L'entra\xeenement a \xe9t\xe9 ex\xe9cut\xe9 dans un environnement Kaggle pour b\xe9n\xe9ficier de l'acc\xe9l\xe9ration GPU :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'with mlflow.start_run(run_name="yolov8m_custom_pretrained") as run:\r\n    # Log des hyperparam\xe8tres\r\n    mlflow.log_param("model_path", "/kaggle/input/modellast43/tensorflow2/default/1/last.pt")\r\n    mlflow.log_param("epochs", 100)\r\n    mlflow.log_param("imgsz", 800)\r\n    \r\n    # Lancement de l\'entra\xeenement YOLOv8\r\n    !yolo task=detect mode=train \\\r\n        model=/kaggle/input/modellast43/tensorflow2/default/1/last.pt \\\r\n        data=/kaggle/working/datasets/Stade_Pheno_Dataset-3/data.yaml \\\r\n        epochs=100 imgsz=800 resume=True\r\n\n'})}),"\n",(0,s.jsx)(n.h3,{id:"-suivi-des-performances-",children:"\ud83d\udcca Suivi des performances :"}),"\n",(0,s.jsx)(n.p,{children:"Pour assurer une tra\xe7abilit\xe9 compl\xe8te, nous avons captur\xe9 les m\xe9triques cl\xe9s \xe0 la fin de l'entra\xeenement :"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:'# Extraction et log des m\xe9triques\r\nresult_file = \'/kaggle/working/datasets/runs/detect/train/results.csv\'\r\nif os.path.exists(result_file):\r\n    df = pd.read_csv(result_file)\r\n    last_row = df.iloc[-1]\r\n    for metric_name, value in last_row.items():\r\n        cleaned_metric_name = metric_name.replace("(", "_").replace(")", "_")\r\n        mlflow.log_metric(cleaned_metric_name, value)\r\n\r\n# Log des artifacts (mod\xe8les, courbes, etc.)\r\nmlflow.log_artifacts(\'/kaggle/working/datasets/runs/detect/train\')\r\n\n'})})]})}function u(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(c,{...e})}):c(e)}}}]);